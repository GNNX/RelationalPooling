{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import deepchem as dc\n",
    "from deepchem.models.tensorgraph.models.graph_models import GraphConvTensorGraph\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TASK = 'tox_21'\n",
    "K = 20\n",
    "technique = 'dfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TASK == 'tox_21':\n",
    "    from deepchem.molnet import load_tox21 as dataloader\n",
    "    NUM_TASKS = 12\n",
    "elif TASK == 'hiv':\n",
    "    from deepchem.molnet import load_hiv as dataloader\n",
    "    NUM_TASKS = 1\n",
    "elif TASK == 'muv':\n",
    "    from deepchem.molnet import load_muv as dataloader\n",
    "    NUM_TASKS = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.tensor_graph import TensorGraph\n",
    "tg = TensorGraph(use_queue=False)\n",
    "from deepchem.models.tensorgraph.layers import Feature\n",
    "from deepchem.models.tensorgraph.layers import Dense, GraphConv, BatchNorm\n",
    "from deepchem.models.tensorgraph.layers import GraphPool, GraphGather\n",
    "from deepchem.models.tensorgraph.layers import Dense, SoftMax, SoftMaxCrossEntropy, WeightedError, Stack\n",
    "from deepchem.models.tensorgraph.layers import Label, Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Tox21 dataset\n",
    "tox21_tasks, tox21_datasets, transformers = dataloader(featurizer='GraphConv',reload=True,split='random')\n",
    "train_dataset, valid_dataset, test_dataset = tox21_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_perm(a):\n",
    "    ordering = list(range(a))\n",
    "    shuffle(ordering)\n",
    "    return ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depth_first_search(neighbour_list, root_node):\n",
    "    visited_nodes = set()\n",
    "    order = []\n",
    "    stack = [root_node]\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited_nodes:\n",
    "            visited_nodes.add(node)\n",
    "            order.append(node)\n",
    "            stack.extend(set(neighbour_list[node]) - visited_nodes)\n",
    "    return order\n",
    "\n",
    "def breadth_first_search(neighbour_list, root_node):\n",
    "    visited_nodes = set()\n",
    "    order = []\n",
    "    queue = [root_node]\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        if node not in visited_nodes:\n",
    "            visited_nodes.add(node)\n",
    "            order.append(node)\n",
    "            queue.extend(set(neighbour_list[node]) - visited_nodes)\n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atom_features = Feature(shape=(None, 75))\n",
    "degree_slice = Feature(shape=(None, 2), dtype=tf.int32)\n",
    "membership = Feature(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "deg_adjs = []\n",
    "for i in range(0, 10 + 1):\n",
    "    deg_adj = Feature(shape=(None, i + 1), dtype=tf.int32)\n",
    "    deg_adjs.append(deg_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "\n",
    "gc1 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[atom_features, degree_slice, membership] + deg_adjs)\n",
    "batch_norm1 = BatchNorm(in_layers=[gc1])\n",
    "gp1 = GraphPool(in_layers=[batch_norm1, degree_slice, membership] + deg_adjs)\n",
    "gc2 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[gp1, degree_slice, membership] + deg_adjs)\n",
    "batch_norm2 = BatchNorm(in_layers=[gc2])\n",
    "gp2 = GraphPool(in_layers=[batch_norm2, degree_slice, membership] + deg_adjs)\n",
    "dense = Dense(out_channels=128, activation_fn=tf.nn.relu, in_layers=[gp2])\n",
    "batch_norm3 = BatchNorm(in_layers=[dense])\n",
    "readout = GraphGather(\n",
    "    batch_size=batch_size,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    in_layers=[batch_norm3, degree_slice, membership] + deg_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costs = []\n",
    "labels = []\n",
    "for task in range(len(tox21_tasks)):\n",
    "    classification = Dense(\n",
    "        out_channels=2, activation_fn=None, in_layers=[readout])\n",
    "\n",
    "    softmax = SoftMax(in_layers=[classification])\n",
    "    tg.add_output(softmax)\n",
    "\n",
    "    label = Label(shape=(None, 2))\n",
    "    labels.append(label)\n",
    "    cost = SoftMaxCrossEntropy(in_layers=[label, classification])\n",
    "    costs.append(cost)\n",
    "all_cost = Stack(in_layers=costs, axis=1)\n",
    "weights = Weights(shape=(None, len(tox21_tasks)))\n",
    "loss = WeightedError(in_layers=[all_cost, weights])\n",
    "tg.set_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_new_X(dataset, K, technique):\n",
    "    count = 0\n",
    "    new_array = []\n",
    "    size = dataset.shape[0]\n",
    "    for i in range(size):\n",
    "        mol = dataset[i]\n",
    "        min_degree, max_degree = 1000, 0\n",
    "        atom_feats = mol.get_atom_features()\n",
    "        adjacent_list = mol.get_adjacency_list()\n",
    "        num_atoms = mol.get_num_atoms()\n",
    "        if num_atoms > K:\n",
    "            #Reduce to k-ary\n",
    "            count+=1\n",
    "            ordering = randomize_perm(num_atoms)\n",
    "            if technique == 'dfs':\n",
    "                order = depth_first_search(adjacent_list,ordering[0])\n",
    "            elif technique == 'bfs':\n",
    "                order = breadth_first_search(adjacent_list,ordering[0])\n",
    "            else :\n",
    "                order = ordering\n",
    "            if (len(order) < K):\n",
    "                  order = ordering\n",
    "            order = order[:K]\n",
    "            atom_feats = atom_feats[order]\n",
    "            new_atom_feats = atom_feats\n",
    "            create_adjacency = []\n",
    "            for i in order:\n",
    "                edges = []\n",
    "                for neighbor in adjacent_list[i]:\n",
    "                    if neighbor in order:\n",
    "                        get_new_index = int(order.index(neighbor))\n",
    "                        edges.append(get_new_index)\n",
    "                create_adjacency.append(edges)\n",
    "            new_mol = dc.feat.mol_graphs.ConvMol(new_atom_feats, create_adjacency)\n",
    "        else :\n",
    "            new_mol = dc.feat.mol_graphs.ConvMol(atom_feats, adjacent_list)\n",
    "        new_array.append(new_mol)\n",
    "    print(count)\n",
    "    return np.array(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK != 'tox_21':\n",
    "    new_train_data = generate_new_X(train_dataset.X, K, technique)\n",
    "    new_train_dataset = dc.data.datasets.DiskDataset.from_numpy(new_train_data, train_dataset.y, train_dataset.w ,train_dataset.ids, data_dir=None)\n",
    "    print(\"Train Data - added RP\")\n",
    "    new_valid_data = generate_new_X(valid_dataset.X, K, technique)\n",
    "    new_valid_dataset = dc.data.datasets.DiskDataset.from_numpy(new_valid_data, valid_dataset.y, valid_dataset.w ,valid_dataset.ids, data_dir=None)\n",
    "    print(\"Valid Data - added RP\")\n",
    "    new_test_data = generate_new_X(test_dataset.X, K, technique)\n",
    "    new_test_dataset = dc.data.datasets.DiskDataset.from_numpy(new_test_data, test_dataset.y, test_dataset.w ,test_dataset.ids, data_dir=None)\n",
    "    print(\"Test Data - added RP\")\n",
    "else :\n",
    "    new_train_data = generate_new_X(train_dataset.X[:3800], K, technique)\n",
    "    new_train_dataset = dc.data.datasets.DiskDataset.from_numpy(new_train_data, train_dataset.y[:3800], train_dataset.w[:3800] ,train_dataset.ids[:3800], data_dir=None)\n",
    "    print(\"Train Data - added RP - tox21\")\n",
    "    new_valid_data = generate_new_X(train_dataset.X[3800:5000], K, technique)\n",
    "    new_valid_dataset = dc.data.datasets.DiskDataset.from_numpy(new_valid_data, train_dataset.y[3800:5000], train_dataset.w[3800:5000] ,train_dataset.ids[3800:5000], data_dir=None)\n",
    "    print(\"Valid Data - added RP - tox21\")\n",
    "    new_test_data = generate_new_X(train_dataset.X[5000:], K, technique)\n",
    "    new_test_dataset = dc.data.datasets.DiskDataset.from_numpy(new_test_data, train_dataset.y[5000:], train_dataset.w[5000:] ,train_dataset.ids[5000:], data_dir=None)\n",
    "    print(\"Test Data - added RP - tox21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(dataset, epochs=1, predict=False, pad_batches=True):\n",
    "    for epoch in range(epochs):\n",
    "        if not predict:\n",
    "            print('Starting epoch %i' % epoch)\n",
    "        for ind, (X_b, y_b, w_b, ids_b) in enumerate(\n",
    "            dataset.iterbatches(batch_size, pad_batches=pad_batches, deterministic=True)):\n",
    "            d = {}\n",
    "            for index, label in enumerate(labels):\n",
    "                d[label] = to_one_hot(y_b[:, index])\n",
    "            d[weights] = w_b\n",
    "            multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
    "            d[atom_features] = multiConvMol.get_atom_features()\n",
    "            d[degree_slice] = multiConvMol.deg_slice\n",
    "            d[membership] = multiConvMol.membership\n",
    "            for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "                d[deg_adjs[i - 1]] = multiConvMol.get_deg_adjacency_lists()[i]\n",
    "            yield d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.fit_generator(data_generator(new_train_dataset, epochs=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "def reshape_y_pred(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    TensorGraph.Predict returns a list of arrays, one for each output\n",
    "    We also have to remove the padding on the last batch\n",
    "    Metrics taks results of shape (samples, n_task, prob_of_class)\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    retval = np.stack(y_pred, axis=1)\n",
    "    return retval[:n_samples]\n",
    "\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_predictions = tg.predict_on_generator(data_generator(new_train_dataset, predict=True))\n",
    "train_predictions = reshape_y_pred(new_train_dataset.y, train_predictions)\n",
    "train_scores = metric.compute_metric(new_train_dataset.y, train_predictions, new_train_dataset.w)\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores)\n",
    "\n",
    "valid_predictions = tg.predict_on_generator(data_generator(new_valid_dataset, predict=True))\n",
    "valid_predictions = reshape_y_pred(new_valid_dataset.y, valid_predictions)\n",
    "valid_scores = metric.compute_metric(new_valid_dataset.y, valid_predictions, new_valid_dataset.w)\n",
    "print(\"Valid ROC-AUC Score: %f\" % valid_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = tg.predict_on_generator(data_generator(new_test_dataset, predict=True))\n",
    "test_predictions = reshape_y_pred(new_test_dataset.y, test_predictions)\n",
    "test_scores = metric.compute_metric(new_test_dataset.y, test_predictions, new_test_dataset.w)\n",
    "print(\"test ROC-AUC Score: %f\" % test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
