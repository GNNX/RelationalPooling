{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import deepchem as dc\n",
    "from itertools import permutations, product\n",
    "from scipy import sparse\n",
    "from random import shuffle\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TASK = 'tox_21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TASK == 'tox_21':\n",
    "    from deepchem.molnet import load_tox21 as dataloader\n",
    "    NUM_TASKS = 12\n",
    "elif TASK == 'hiv':\n",
    "    from deepchem.molnet import load_hiv as dataloader\n",
    "    NUM_TASKS = 1\n",
    "elif TASK == 'muv':\n",
    "    from deepchem.molnet import load_muv as dataloader\n",
    "    NUM_TASKS = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_adjacency(a, b):\n",
    "    data0 = a.get_atom_features()\n",
    "    data1 = b.get_atom_features()\n",
    "    index = list(range(data0.shape[0]))\n",
    "    remain = list(range(data1.shape[0]))\n",
    "    mapping_dict = dict((k,k) for k in index)\n",
    "    for i in index :\n",
    "        for j in remain:\n",
    "            if np.array_equal(data0[i],data1[j]):\n",
    "                mapping_dict[i] = j\n",
    "                remain.remove(j)\n",
    "                break\n",
    "    return mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_perm(a):\n",
    "    ordering = list(range(a))\n",
    "    shuffle(ordering)\n",
    "    return ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def permute_array(a, ordering, mapping_dict):\n",
    "    pair_features = a.get_pair_features()\n",
    "    new_array = np.zeros(pair_features.shape)\n",
    "    mod_factor = pair_features.shape[0]\n",
    "    m,n = 0,0\n",
    "    for i in ordering:\n",
    "        for j in ordering:\n",
    "            new_array[m][n] = pair_features[mapping_dict[i]][mapping_dict[j]]\n",
    "            n+=1\n",
    "            n = n%mod_factor\n",
    "        m+=1\n",
    "        m = m%mod_factor\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depth_first_search(neighbour_list, root_node):\n",
    "    visited_nodes = set()\n",
    "    order = []\n",
    "    stack = [root_node]\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited_nodes:\n",
    "            visited_nodes.add(node)\n",
    "            order.append(node)\n",
    "            stack.extend(set(neighbour_list[node]) - visited_nodes)\n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_tensor(dataset_conv, dataset_weave, y):\n",
    "    size = dataset_weave.shape[0]\n",
    "    arr_pair = []\n",
    "    arr_indv = []\n",
    "    y_true = []\n",
    "    for i in range(size):\n",
    "        example = []\n",
    "        a = dataset_conv[i]\n",
    "        b = dataset_weave[i]\n",
    "        ordering = randomize_perm(a.get_num_atoms())\n",
    "        mapping_dict = align_adjacency(a, b)\n",
    "        order = depth_first_search(a.get_adjacency_list(),ordering[0])\n",
    "        pair_array = permute_array(b, order, mapping_dict)\n",
    "        indv_array = a.get_atom_features()[order]\n",
    "        if len(pair_array) == len(indv_array) :\n",
    "            arr_pair.append(torch.Tensor(pair_array).to(device))\n",
    "            arr_indv.append(torch.Tensor(indv_array).to(device))\n",
    "            y_true.append(y[i])\n",
    "    return (arr_indv, arr_pair, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unison_shuffled(a, b, c):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn_unit_1 = nn.LSTM(14, 100, batch_first=True)\n",
    "        self.indv_linear_1 = nn.Linear(75, 100)\n",
    "        init.xavier_uniform_(self.indv_linear_1.weight)\n",
    "        self.indv_act_1 = nn.ReLU()\n",
    "        self.rnn_unit_2 = nn.LSTM(200,100, batch_first=True)\n",
    "        self.rho_lin_1 = nn.Linear(100,100)\n",
    "        init.xavier_uniform_(self.rho_lin_1.weight)\n",
    "        self.rho_act_1 = nn.ReLU()\n",
    "        self.final_lin = nn.Linear(100,NUM_TASKS)\n",
    "        init.xavier_uniform_(self.final_lin.weight)\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, pair_inp, indv_inp):\n",
    "        rho_input = torch.zeros((1,100)).to(device)\n",
    "        for i in range(len(pair_inp)):\n",
    "            out_rnn_1,(h_n,c_n) = self.rnn_unit_1(pair_inp[i])\n",
    "            out_indv = self.indv_linear_1(indv_inp[i])\n",
    "            out_indv = self.indv_act_1(out_indv).unsqueeze(0)\n",
    "            inp_rnn_2 = torch.cat((c_n, out_indv),2)\n",
    "            out_rnn_2,(h_n,c_n) = self.rnn_unit_2(inp_rnn_2)\n",
    "            c_n = c_n.squeeze(0)\n",
    "            rho_input = torch.cat((rho_input,c_n),0)\n",
    "        rho_input = rho_input[1:]\n",
    "        rho_out = self.rho_lin_1(rho_input)\n",
    "        rho_out = self.rho_act_1(rho_out)\n",
    "        final_out = self.final_lin(rho_out)\n",
    "        return final_out\n",
    "        \n",
    "    def compute_loss(self, pair_inp, indv_inp, y_true):\n",
    "        pred = self.forward(pair_inp, indv_inp)\n",
    "        return self.loss_func(pred, y_true)\n",
    "    \n",
    "    def compute_proba(self, pair_inp, indv_inp):\n",
    "        return torch.sigmoid(self.forward(pair_inp, indv_inp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tox21_tasks_weave, tox21_datasets_weave, transformers_weave = dataloader(featurizer='Weave')\n",
    "tox21_tasks_conv, tox21_datasets_conv, transformers_conv = dataloader(featurizer='GraphConv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_shuffled_conv = train_dataset_conv.X\n",
    "train_shuffled_weave = train_dataset_weave.X\n",
    "train_shuffled_y = train_dataset_conv.y\n",
    "\n",
    "if TASK != 'tox_21':\n",
    "    train_pair = train_dataset_weave.X\n",
    "    train_indv = train_dataset_conv.X\n",
    "    train_y = train_dataset_conv.y\n",
    "    valid_pair = valid_dataset_weave.X\n",
    "    valid_indv = valid_dataset_conv.X\n",
    "    valid_y = valid_dataset_conv.y\n",
    "    test_pair = test_dataset_weave.X\n",
    "    test_indv = test_dataset_conv.X\n",
    "    test_y = test_dataset_conv.y \n",
    "else :\n",
    "    train_shuffled_conv, train_shuffled_weave, train_shuffled_y = unison_shuffled(train_shuffled_conv, train_shuffled_weave, train_shuffled_y)\n",
    "    train_indv = train_shuffled_conv[:3800]\n",
    "    train_pair = train_shuffled_weave[:3800]\n",
    "    train_y = train_shuffled_y[:3800]\n",
    "    valid_indv = train_shuffled_conv[3800:5000]\n",
    "    valid_pair = train_shuffled_weave[3800:5000]\n",
    "    valid_y = train_shuffled_y[3800:5000]\n",
    "    test_indv = train_shuffled_conv[5000:]\n",
    "    test_pair = train_shuffled_weave[5000:]\n",
    "    test_y = train_shuffled_y[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_shuffled_conv, train_shuffled_weave, train_shuffled_y = unison_shuffled(train_shuffled_conv, train_shuffled_weave, train_shuffled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct Valid and Test \n",
    "indv, pair, y_true = construct_tensor(train_indv,train_pair, train_y)\n",
    "valid_indv, valid_pair, valid_y_true = construct_tensor(valid_indv, valid_pair, valid_y)\n",
    "test_indv, test_pair, test_y_true = construct_tensor(test_indv, test_pair, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train over multiple epochs\n",
    "val_score_tracker, train_loss_tracker = [],[]\n",
    "NUM_TRAINING_EXAMPLES = len(indv)\n",
    "start_time = time.time()\n",
    "num_batches = int(NUM_TRAINING_EXAMPLES / batch_size)\n",
    "num_epochs = 50\n",
    "val_loss_tracker = []\n",
    "num_steps_tracker = []\n",
    "checkpoint_file_name = \"rnn_dfs_tox21.model\"\n",
    "checker = RNNModel().to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, checker.parameters()), lr=0.003)\n",
    "count = 0\n",
    "best_roc_auc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch Num: \", epoch)\n",
    "    # Do seed and random shuffle of the input\n",
    "    print(\"Performing Random Shuffle\")\n",
    "    train_indv, train_pair, train_y = unison_shuffled(train_indv,train_pair, train_y)\n",
    "    indv, pair, y_true = construct_tensor(train_indv,train_pair, train_y)\n",
    "    y_true_tensor = torch.FloatTensor(y_true).to(device)\n",
    "    print(\"Random Shuffle Done\")\n",
    "    for batch in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_pair = pair[batch_size * batch:batch_size * batch + batch_size]\n",
    "        batch_indv = indv[batch_size * batch:batch_size * batch + batch_size]\n",
    "        batch_y = y_true_tensor[batch_size * batch:batch_size * batch + batch_size]\n",
    "        loss = checker.compute_loss(batch_pair, batch_indv, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count+=1\n",
    "        \n",
    "        if(count%100 == 0):\n",
    "            with torch.no_grad():\n",
    "                val_loss = checker.compute_loss(valid_pair, valid_indv, torch.FloatTensor(valid_y_true).to(device))\n",
    "                val_loss_tracker.append(val_loss.item())\n",
    "                pickle.dump(val_loss_tracker, open( \"val_loss_dfs_rnn_tox21.p\", \"wb\" ))\n",
    "                print(\"Val Loss at Step \", count, \" : \", val_loss.item())\n",
    "                num_steps_tracker.append(count)\n",
    "\n",
    "                val_out = checker.compute_proba(valid_pair, valid_indv)\n",
    "                val_y_pred = np.round(val_out.detach().cpu().numpy())\n",
    "                val_score = roc_auc_score(np.array(valid_y_true), val_y_pred)\n",
    "                val_score_tracker.append(val_score)\n",
    "                pickle.dump(val_score_tracker, open( \"val_score_dfs_rnn_tox21.p\", \"wb\" ))\n",
    "                if val_score > best_roc_auc :\n",
    "                    print(\"Best Val ROC AUC Score till now: \", val_score)\n",
    "                    best_roc_auc = val_score\n",
    "                    torch.save(checker.state_dict(),checkpoint_file_name)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = checker.compute_loss(pair, indv, y_true_tensor)\n",
    "        print(\"Epoch Training Loss: \", loss.item())\n",
    "        train_loss_tracker.append(loss.item())\n",
    "        pickle.dump(train_loss_tracker, open( \"train_loss_dfs_rnn_tox21.p\", \"wb\" ))\n",
    "        \n",
    "\n",
    "end_time = time.time()\n",
    "total_training_time = end_time - start_time\n",
    "print(\"Total Time: \", total_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
